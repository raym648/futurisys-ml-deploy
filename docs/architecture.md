# Architecture du SystÃ¨me

## ğŸ—ï¸ Vue dâ€™ensemble

Lâ€™architecture du projet **Futurisys ML Deploy** 
repose sur une sÃ©paration claire et conforme 
aux bonnes pratiques **MLOps** entre :

* API (serving & orchestration)
* Couche Machine Learning (prÃ©diction uniquement)
* Base de donnÃ©es (traÃ§abilitÃ©)
* CI/CD (qualitÃ© & dÃ©ploiement)
* Dashboard (monitoring & analyse)

Cette architecture est conÃ§ue pour Ãªtre 
**testable, dÃ©ployable automatiquement et auditable**.

---

## ğŸ”Œ Composants

### ğŸš€ API FastAPI

RÃ´le principal : **exposition du modÃ¨le ML en production**.

FonctionnalitÃ©s :

* Point dâ€™entrÃ©e unique (`/`)
* Endpoint de prÃ©diction (`/predict`)
* Registry de modÃ¨les (`/models`)
* AccÃ¨s aux mÃ©tadonnÃ©es (`/metadata`)
* Exposition des mÃ©triques (`/metrics`)
* Documentation Swagger automatique

ğŸ‘‰ Lâ€™API ne contient **aucune logique dâ€™entraÃ®nement ML**.

---

### ğŸ§  Couche Machine Learning

ResponsabilitÃ© : **infÃ©rence uniquement**.

CaractÃ©ristiques :

* Chargement paresseux (lazy loading) des modÃ¨les
* Registry centralisÃ© des modÃ¨les disponibles
* ModÃ¨le par dÃ©faut configurable
* Gestion des erreurs et fallback en cas de modÃ¨le invalide

Artefacts :

* ModÃ¨les sÃ©rialisÃ©s (`.joblib`)
* Fichiers de features et paramÃ¨tres

ğŸ‘‰ Aucun rÃ©-entraÃ®nement en production.

---

### ğŸ—„ï¸ Base de donnÃ©es PostgreSQL (Neon)

RÃ´le : **traÃ§abilitÃ© complÃ¨te des prÃ©dictions**.

Stockage :

* Dataset source
* Inputs envoyÃ©s au modÃ¨le
* Outputs de prÃ©diction
* Identifiants de requÃªtes

ğŸ‘‰ Permet audit, monitoring et analyse a posteriori.

---

### ğŸ” CI/CD â€“ GitHub Actions

Pipeline automatisÃ© exÃ©cutÃ© Ã  chaque `push` et `pull request`.

Ã‰tapes principales :

* Installation des dÃ©pendances
* Linting (flake8)
* Tests unitaires et fonctionnels (pytest)
* Smoke tests FastAPI
* GÃ©nÃ©ration du rapport de couverture
* DÃ©ploiement automatique sur Hugging Face Spaces (branche `main`)

ğŸ‘‰ En environnement CI (`ENV=test`), 
les modÃ¨les ML sont **mockÃ©s** pour garantir des tests rapides 
et reproductibles.

---

### ğŸ“Š Dashboard Streamlit

FonctionnalitÃ©s :

* Visualisation des mÃ©triques
* Analyse des prÃ©dictions
* Comparaison des modÃ¨les

ğŸ‘‰ ConnectÃ© Ã  lâ€™API et/ou Ã  la base de donnÃ©es.

---

## ğŸ”„ Flux de donnÃ©es

1. RequÃªte utilisateur vers lâ€™API
2. Validation et prÃ©paration des features
3. Enregistrement de lâ€™input en base
4. PrÃ©diction via le modÃ¨le ML
5. Enregistrement de lâ€™output en base
6. RÃ©ponse structurÃ©e Ã  lâ€™utilisateur

---

## ğŸ“Œ Avantages de lâ€™architecture

* âœ… SÃ©paration claire des responsabilitÃ©s
* âœ… TraÃ§abilitÃ© complÃ¨te des prÃ©dictions
* âœ… Tests CI indÃ©pendants des artefacts ML
* âœ… ScalabilitÃ© et maintenabilitÃ©
* âœ… Alignement avec les bonnes pratiques MLOps
