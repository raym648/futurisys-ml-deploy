# Monitoring & ObservabilitÃ©

## ğŸ¯ Objectifs

Le monitoring vise Ã  garantir :

* la fiabilitÃ© des prÃ©dictions en production
* la dÃ©tection prÃ©coce des dÃ©rives (data drift & concept drift)
* la transparence des performances du modÃ¨le
* la traÃ§abilitÃ© des dÃ©cisions ML

---

## ğŸ“Š MÃ©triques suivies

Les mÃ©triques sont calculÃ©es horsâ€‘ligne aprÃ¨s entraÃ®nement et comparÃ©es entre versions de modÃ¨les :

* Accuracy
* Precision
* Recall
* F1â€‘score
* Matrice de confusion
* Distribution des prÃ©dictions (classe 0 / classe 1)

Chaque fichier de mÃ©triques est versionnÃ© par modÃ¨le.

---

## ğŸ—„ï¸ Stockage des mÃ©triques

* CalculÃ©es offline (notebook / pipeline ML)
* StockÃ©es sous forme de fichiers CSV
* Localisation : `data/ml_artifacts/metrics/`
* Un fichier CSV par modÃ¨le et par version

Les mÃ©triques sont exposÃ©es via lâ€™API REST :

* `GET /metrics` â†’ liste des fichiers disponibles
* `GET /metrics/{filename}` â†’ contenu dÃ©taillÃ© dâ€™un fichier

---

## ğŸ“ˆ Dashboard

Le dashboard Streamlit permet :

* la visualisation des performances par modÃ¨le
* la comparaison entre versions
* lâ€™analyse temporelle des rÃ©sultats
* lâ€™aide Ã  la dÃ©cision pour la promotion dâ€™un modÃ¨le en production

---

## ğŸ” ObservabilitÃ© applicative

En complÃ©ment des mÃ©triques ML :

* chaque requÃªte est tracÃ©e (input / output)
* les prÃ©dictions sont historisÃ©es en base de donnÃ©es
* les erreurs sont visibles via les logs applicatifs

---

## ğŸš¨ Perspectives dâ€™amÃ©lioration

* Ajout dâ€™alertes automatiques (seuils de performance)
* DÃ©tection automatique de data drift
* Centralisation des logs (ex : OpenTelemetry)
* Monitoring en temps rÃ©el des distributions dâ€™inputs
