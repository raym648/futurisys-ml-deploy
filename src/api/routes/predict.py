# futurisys-ml-deploy/src/api/routes/predict.py

from fastapi import APIRouter, Query

from src.api.schemas import PredictionRequest, PredictionResponse
from src.data.api_db_integration import record_input, record_output
from src.ml.predictor import predict

router = APIRouter()

# Version globale (fallback)
MODEL_VERSION = "e02-ml-v1"


@router.post("/", response_model=PredictionResponse)
def predict_endpoint(
    payload: PredictionRequest,
    model: str
    | None = Query(
        default=None,
        # fmt: off
        description=(
            "Nom du mod√®le √† utiliser "
            "(dummy, logistic, random_forest, random_forest_e04)"
        ),
        # fmt: on
    ),
):
    """
    Endpoint de pr√©diction ML.
    - Par d√©faut : mod√®le de production
    - Optionnel : ?model=random_forest
    """

    input_data = payload.data

    # S√©paration claire entre tracking et s√©lection du mod√®le
    model_name = model
    model_version = MODEL_VERSION

    # üîµ Enregistrement INPUT
    trace = record_input(payload=input_data, model_version=model_version)

    try:
        # üîµ Pr√©diction
        result = predict(payload=input_data, model_name=model_name)

    except ValueError as exc:
        # Validation douce : mod√®le inconnu ou erreur m√©tier
        result = {
            "prediction": -1,
            "probability": 0.0,
            "error": str(exc),
        }

    # üîµ Enregistrement OUTPUT
    record_output(
        input_id=trace["input_id"],
        request_id=trace["request_id"],
        result=result,
        model_version=model_version,
    )

    return {"request_id": str(trace["request_id"]), **result}
